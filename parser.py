import json
import os.path
import pickle
import time
import pandas as pd

import nltk
from docx import Document
from nltk.corpus.reader.api import CorpusReader, CategorizedCorpusReader
from nltk.corpus.reader.plaintext import CategorizedPlaintextCorpusReader, sent_tokenize
from nltk import wordpunct_tokenize, pos_tag
from readability.readability import Unparseable
from readability.readability import Document as Paper
import bs4

# import nltk

# nltk.download('punkt')
# nltk.download('averaged_perceptron_tagger')
# nltk.download('averaged_perceptron_tagger_ru')

CAT_PATTERN = r'([\w_\s\.]+)/.*'
DOC_PATTERN = r'(?!\.)[\w_\s]+/[\w\s\d\-]+\.txt'
# HTML_PATTERN = r'(?!\.)[/\w_\s]+[/\w+.-]*[\.html]+'
HTML_PATTERN = r'.*[\.html]+'
# PKL_PATTERN = r'(?!\.)[a-z_\s]+/[a-f0-9]+\.pickle'
PKL_PATTERN = r'.*[\.pickle]+'
TAGS = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'h7', 'p', 'li']


# TODO: –£–±—Ä–∞—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã –≤ –∫–æ–¥–µ, –≤—ã–¥–µ–ª–∏—Ç—å –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏.
#  P.S. –ê–∫–∫—É—Ä–∞—Ç–Ω–æ —Å –Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ–º!

def iscyrillic(s):
    cyrillic_symbols = set('–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è-')
    word_symbols = set(s.lower())
    return word_symbols.difference(cyrillic_symbols) == set()


class TXTCorpusReader(CategorizedPlaintextCorpusReader):
    def __init__(self, root, fileids=DOC_PATTERN, encoding='utf8', **kwargs):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–∞ –¥–ª—è —á—Ç–µ–Ω–∏—è TXT-—Ñ–∞–π–ª–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å—Ä–µ–¥—Å—Ç–≤ NLTK.

        :param root: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
        :param fileids: –®–∞–±–ª–æ–Ω –ø–æ –∫–æ—Ç–æ—Ä–æ–º—É –∏–∑–≤–ª–µ–∫–∞—é—Ç—Å—è –¥–æ–∫—É–º–µ–Ω—Ç—ã
        :param encoding: –ö–æ–¥–∏—Ä–æ–≤–∫–∞, –≤ –∫–æ—Ç–æ—Ä–æ–π —Å—á–∏—Ç—ã–≤–∞—é—Ç—Å—è —Ñ–∞–π–ª—ã
        :param kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        """
        if not any(key.startswith('cat_') for key in kwargs.keys()):
            kwargs['cat_pattern'] = CAT_PATTERN
        CategorizedPlaintextCorpusReader.__init__(self, root, fileids, encoding, **kwargs)
        # CorpusReader.__init__(self, root, fileids, encoding)

    def resolve(self, fileids=None, categories=None):
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ —Ñ–∞–π–ª–æ–≤ –∏–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–π –∫–∞—Ç–µ–≥–æ—Ä–∏–π,
        –∫–æ—Ç–æ—Ä—ã–µ –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è –∫–∞–∂–¥–æ–π –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π —Ñ—É–Ω–∫—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–∞ —á—Ç–µ–Ω–∏—è –∫–æ—Ä–ø—É—Å–∞.
        """
        # if fileids is not None and categories is not None:
        #     raise ValueError("–£–∫–∞–∂–∏—Ç–µ id-—Ñ–∞–π–ª–æ–≤ –∏–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–≤—ã–±–µ—Ä–∏—Ç–µ –æ–¥–∏–Ω –≤–∞—Ä–∏–∞–Ω—Ç)")
        if categories is not None:
            return self.fileids(categories)
        return fileids

    def docs(self, fileids=None, categories=None):
        """
        –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–µ—Å—å —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∑–∞–∫—Ä—ã–≤–∞—è –µ–≥–æ –ø–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—é —á—Ç–µ–Ω–∏—è.
        """
        fileids = self.resolve(fileids, categories)

        for path, encoding in self.abspaths(fileids, include_encoding=True):
            with open(path, 'r', encoding=encoding) as f:
                yield f.read()

    def sizes(self, fileids=None, categories=None):
        """
        –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä, –≤–æ–∑–≤—Ä–∞—â–∞—é—â–∏–π –∏–º–µ–Ω–∞ –∏ —Ä–∞–∑–º–µ—Ä—ã —Ñ–∞–π–ª–æ–≤.
        """
        fileids = self.resolve(fileids, categories)

        for path in self.abspaths(fileids):
            yield path, os.path.getsize(path)

    def paras(self, fileids=None, categories=None):
        """
        –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è –∞–±–∑–∞—Ü–µ–≤ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
        """
        fileids = self.resolve(fileids, categories)
        for doc in self.docs(fileids, categories):
            lines = doc.split('\n')
            for line in lines:
                if line != '':
                    yield line

    def sents(self, fileids=None, categories=None):
        """
        –í—ã–¥–µ–ª—è–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–∑ –∞–±–∑–∞—Ü–µ–≤.
        """
        for paragraph in self.paras(fileids, categories):
            for sentence in sent_tokenize(paragraph):
                yield sentence

    def words(self, fileids=None, categories=None):
        """
        –í—ã–¥–µ–ª—è–µ—Ç —Å–ª–æ–≤–∞ –∏–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.
        """
        for sentence in self.sents(fileids, categories):
            for token in wordpunct_tokenize(sentence):
                yield token

    def tokenize(self, fileids=None, categories=None):
        for paragraph in self.paras(fileids, categories):
            yield [pos_tag(wordpunct_tokenize(sent), lang='rus') for sent in sent_tokenize(paragraph)]


class HTMLCorpusReader(CategorizedCorpusReader, CorpusReader):
    """
    –û–±—ä–µ–∫—Ç –¥–ª—è —á—Ç–µ–Ω–∏—è –∫–æ—Ä–ø—É—Å–∞ —Å HTML-–¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏
    –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏.
    """

    def __init__(self, root, fileids=HTML_PATTERN, encoding='utf8', tags=TAGS, **kwargs):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–∞ –¥–ª—è —á—Ç–µ–Ω–∏—è HTML-—Ñ–∞–π–ª–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å—Ä–µ–¥—Å—Ç–≤ NLTK.

        :param root: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
        :param fileids: –®–∞–±–ª–æ–Ω –ø–æ –∫–æ—Ç–æ—Ä–æ–º—É –∏–∑–≤–ª–µ–∫–∞—é—Ç—Å—è –¥–æ–∫—É–º–µ–Ω—Ç—ã
        :param encoding: –ö–æ–¥–∏—Ä–æ–≤–∫–∞, –≤ –∫–æ—Ç–æ—Ä–æ–π —Å—á–∏—Ç—ã–≤–∞—é—Ç—Å—è —Ñ–∞–π–ª—ã
        :param tags: HTML —Ç–µ–≥–∏ –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–µ—Å—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞
        :param kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        """
        if not any(key.startswith('cat_') for key in kwargs.keys()):
            kwargs['cat_pattern'] = CAT_PATTERN

        # TXTCorpusReader.__init__(self, root, fileids, encoding, **kwargs)
        CategorizedCorpusReader.__init__(self, kwargs)
        CorpusReader.__init__(self, root, fileids, encoding)
        self.tags = tags

    def resolve(self, fileids=None, categories=None):
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ —Ñ–∞–π–ª–æ–≤ –∏–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–π –∫–∞—Ç–µ–≥–æ—Ä–∏–π,
        –∫–æ—Ç–æ—Ä—ã–µ –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è –∫–∞–∂–¥–æ–π –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π —Ñ—É–Ω–∫—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–∞ —á—Ç–µ–Ω–∏—è –∫–æ—Ä–ø—É—Å–∞.
        """
        if fileids is not None and categories is not None:
            raise ValueError("–£–∫–∞–∂–∏—Ç–µ id-—Ñ–∞–π–ª–æ–≤ –∏–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–≤—ã–±–µ—Ä–∏—Ç–µ –æ–¥–∏–Ω –≤–∞—Ä–∏–∞–Ω—Ç)")
        if categories is not None:
            return self.fileids(categories)
        if fileids is not None:
            return fileids
        return self.fileids()

    def docs(self, fileids=None, categories=None):
        """
        –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–µ—Å—å —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∑–∞–∫—Ä—ã–≤–∞—è –µ–≥–æ –ø–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—é —á—Ç–µ–Ω–∏—è.
        """
        fileids = self.resolve(fileids, categories)

        for path, encoding in self.abspaths(fileids, include_encoding=True):
            with open(path, 'r', encoding=encoding) as f:
                yield f.read()

    def html(self, fileids=None, categories=None):
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ HTML –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –æ—á–∏—â–∞—è –∏—Ö —Å –ø–æ–º–æ—â—å—é –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ readability-lxml.
        """
        for doc in self.docs(fileids, categories):
            try:
                yield Paper(doc).summary()
            except Unparseable as e:
                print(f'–ù–µ –º–æ–≥—É —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å HTML-—Å—Ç—Ä–∞–Ω–∏—Ü—É: {e}')
                continue

    def paras(self, fileids=None, categories=None):
        """
        –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è –∞–±–∑–∞—Ü–µ–≤ –∏–∑ HTML.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫—É BeautifulSoup.
        """
        tags = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'h7', 'p', 'li']
        fileids = self.resolve(fileids, categories)
        for html in self.html(fileids, categories):
            soup = bs4.BeautifulSoup(html, 'lxml')
            for element in soup.find_all(tags):
                yield element.text
            soup.decompose()

    def sents(self, fileids=None, categories=None):
        """
        –í—ã–¥–µ–ª—è–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–∑ –∞–±–∑–∞—Ü–µ–≤.
        """
        for paragraph in self.paras(fileids, categories):
            for sentence in sent_tokenize(paragraph):
                yield sentence

    def words(self, fileids=None, categories=None):
        """
        –í—ã–¥–µ–ª—è–µ—Ç —Å–ª–æ–≤–∞ –∏–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.
        """
        for sentence in self.sents(fileids, categories):
            for token in wordpunct_tokenize(sentence):
                yield token

    def tokenize(self, fileids=None, categories=None):
        for paragraph in self.paras(fileids, categories):
            yield [pos_tag(wordpunct_tokenize(sent), lang='rus') for sent in sent_tokenize(paragraph)]

    # TODO: (*–¥—É–±–ª–∏–∫–∞—Ç—ã*)
    # def token_filter(self, tokens):
    #     return filtered_tokens

    def describe(self, fileids=None, categories=None):
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –æ–±—Ö–æ–¥ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –æ—Ü–µ–Ω–∫–∞–º–∏,
        –æ–ø–∏—Å—ã–≤–∞—é—â–∏–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∫–æ—Ä–ø—É—Å–∞.

        :param (str) fileids: –ù–∞–∑–≤–∞–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω–æ –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        :param (str) categories: –ö–∞—Ç–µ–≥–æ—Ä–∏—è, –∏–∑ –∫–æ—Ç–æ—Ä–æ–π –∏–∑–≤–ª–µ–∫–∞—é—Ç—Å—è –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        :return (dict): –°–ª–æ–≤–∞—Ä—å —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –æ—Ü–µ–Ω–∫–∞–º–∏,
            –æ–ø–∏—Å—ã–≤–∞—é—â–∏–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∫–æ—Ä–ø—É—Å–∞.
        """
        started = time.time()

        counts = nltk.FreqDist()
        tokens = nltk.FreqDist()

        for para in self.paras(fileids, categories):
            counts['paras'] += 1

            # for sent in t_para:
            for sent in sent_tokenize(para):
                counts['sents'] += 1

                # –£–¥–∞–ª–µ–Ω–∏–µ —Å–∏–º–≤–æ–ª–æ–≤
                delete_symbols = ['\n', ',', '.', '!', '?', ':', ';', '_', '#', '(', ')', '\\', '/', '‚á°', '√ó',
                                  '¬´', '¬ª', '', '‚Äî', '<', '>', '+', '[', ']', 'üëé', '‚ùå', 'üáµüáπ', 'üá¶üá∑', 'üü¢', 'üî¥',
                                  '‚ö™Ô∏è', '\u200b', '‚Ä¶', 'üëå', '‚Ä¢\u200e', '"', '\u2009']
                word_tokens = str(sent)
                word_tokens = word_tokens.replace('\xa0', ' ')
                for ds in delete_symbols:
                    word_tokens = word_tokens.replace(ds, '')
                word_tokens = word_tokens.split(' ')

                # for word in wordpunct_tokenize(sent):
                for word in word_tokens:
                    counts['words'] += 1
                    tokens[word] += 1

        n_fileids = len(self.resolve(fileids, categories) or self.fileids)
        n_topics = len(self.categories(self.resolve(fileids, categories)))

        result = {
            'files': n_fileids,
            'topics': n_topics,
            'paras': counts['paras'],
            'sents': counts['sents'],
            'words': counts['words'],
            'vocab': len(tokens),
            'lexdiv': float(counts['words']) / float(len(tokens)),
            'ppdoc': float(counts['paras']) / float(n_fileids),
            'sspar': float(counts['sents']) / float(counts['paras']),
            'secs': time.time() - started
        }
        return result


class Preprocessor(object):
    """
    –û–±–µ—Ä—Ç—ã–≤–∞–µ—Ç 'HTMLCorpusReader' –∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ª–µ–∫—Å–µ–º–∏–∑–∞—Ü–∏—é
    —Å –º–∞—Ä–∫–∏—Ä–æ–≤–∫–æ–π —á–∞—Å—Ç—è–º–∏ —Ä–µ—á–∏.
    """

    def __init__(self, corpus, target='./processed_corpus', **kwargs):
        self.corpus = corpus
        self.target = target

    def fileids(self, fileids=None, categories=None):
        fileids = self.corpus.resolve(fileids, categories)
        if fileids:
            return fileids
        return self.corpus.fileids()

    def abspath(self, fileid):
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∫–æ—Ä–Ω—è —Ü–µ–ª–µ–≤–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞.
        """
        # –ù–∞–π—Ç–∏ –ø—É—Ç—å –∫ –∫–∞—Ç–∞–ª–æ–≥—É –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∫–æ—Ä–Ω—è –∏—Å–∫—Ö–æ–¥–Ω–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞.
        parent = os.path.relpath(os.path.dirname(self.corpus.abspath(fileid)), self.corpus.root)

        # –í—ã–¥–µ–ª–∏—Ç—å —á–∞—Å—Ç–∏ –ø—É—Ç–∏ –¥–ª—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∏—Ä–æ–≤–∞–Ω–∏—è
        basename = os.path.basename(fileid)
        name, ext = os.path.splitext(basename)

        # –°–∫–æ–Ω—Å—Ç—Ä—É–∏—Ä–æ–≤–∞—Ç—å –∏–º—è —Ñ–∞–π–ª–∞ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º .pickle
        basename = name + '.pickle'

        # –í–µ—Ä–Ω—É—Ç—å –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∫–æ—Ä–Ω—è —Ü–µ–ª–µ–≤–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞
        return os.path.normpath(os.path.join(self.target, parent, basename))

    def tokenize(self, fileid):
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å–ª–æ–≤ –∏–∑ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞.
        """
        # TODO: –ø–µ—Ä–µ–¥–µ–ª–∞—Ç—å –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ (–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ)
        for paragraph in self.corpus.paras(fileids=fileid):
            yield [pos_tag(wordpunct_tokenize(sent), lang='rus') for sent in sent_tokenize(paragraph)]

    def process(self, fileid):
        """
        –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞, –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ –Ω–∞ –¥–∏—Å–∫–µ, —á—Ç–æ–±—ã
        –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –æ—à–∏–±–æ–∫, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç +tokenize()+ –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π
        –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –≤ –≤–∏–¥–µ —Å–∂–∞—Ç–æ–≥–æ –∞—Ä—Ö–∏–≤–∞.
        """
        # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–ª—è –∑–∞–ø–∏—Å–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        target = self.abspath(fileid)
        parent = os.path.dirname(target)

        # –£–±–µ–¥–∏—Ç—å—Å—è –≤ —Å—É—â–µ—Å—Ç–æ–≤–∞–Ω–∏–∏ –∫–∞—Ç–∞–ª–æ–≥–∞
        if not os.path.exists(parent):
            os.makedirs(parent)

        # –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ parent - —ç—Ç–æ –∫–∞—Ç–∞–ª–æ–≥, –∞ –Ω–µ —Ñ–∞–π–ª
        if not os.path.isdir(parent):
            raise ValueError('–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ —É–∫–∞–∑–∞–Ω –∫–∞—Ç–∞–ª–æ–≥, –∞ –Ω–µ —Ñ–∞–π–ª')

        # –°–æ–∑–¥–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–ø–∏—Å–∏ –≤ –∞—Ä—Ö–∏–≤
        doc = self.tokenize(fileid)
        document = list(doc)

        # –ó–∞–ø–∏—Å–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ –∞—Ä—Ö–∏–≤ –Ω–∞ –¥–∏—Å–∫
        with open(target, 'wb') as file:
            pickle.dump(document, file, pickle.HIGHEST_PROTOCOL)

        # –£–¥–∞–ª–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –∏–∑ –ø–∞–º—è—Ç–∏
        del document

        # –í–µ—Ä–Ω—É—Ç—å –ø—É—Ç—å –∫ —Ü–µ–ª–µ–≤–æ–º—É —Ñ–∞–π–ª—É
        return target

    def transform(self, fileids=None, categories=None):
        # –°–æ–∑–¥–∞—Ç—å —Ü–µ–ª–µ–≤–æ–π –∫–∞—Ç–∞–ª–æ–≥, –µ—Å–ª–∏ —á–µ–≥–æ –µ—â—ë –Ω–µ—Ç
        if not os.path.exists(self.target):
            os.makedirs(self.target)

        # –ü–æ–ª—É—á–∏—Ç—å –∏–º–µ–Ω–∞ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        for fileid in self.fileids(fileids, categories):
            yield self.process(fileid)

    def save_tokens(self, fileids=None, categories=None):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è —Å–æ —Å–ª–æ–≤–∞–º–∏ –≤ —Ñ–∞–π–ª–µ —Ñ–æ—Ä–º–∞—Ç–∞ JSON.
        """
        tokens = nltk.FreqDist()
        counts = 0
        for para in self.corpus.paras(fileids, categories):
            for sent in sent_tokenize(para):
                # –£–¥–∞–ª–µ–Ω–∏–µ —Å–∏–º–≤–æ–ª–æ–≤
                delete_symbols = ['\n', ',', '.', '!', '?', ':', ';', '_', '#', '(', ')', '\\', '/', '‚á°', '√ó',
                                  '¬´', '¬ª', '', '‚Äî', '<', '>', '+', '%', '[', ']', 'üëé', '‚ùå', 'üáµüáπ', 'üá¶üá∑', 'üü¢',
                                  'üî¥', '‚ö™Ô∏è', '\u200b', '‚Ä¶', 'üëå', '‚Ä¢\u200e', '"', '\u2009', '¬£']
                word_tokens = str(sent)
                word_tokens = word_tokens.replace('\xa0', ' ')
                for ds in delete_symbols:
                    word_tokens = word_tokens.replace(ds, '')
                word_tokens = word_tokens.split(' ')
                counts += 1
                # print(counts)
                for word in word_tokens:
                    if iscyrillic(word) and word != '':
                        tokens[word] += 1

        word_list = {
            "words": list(tokens.keys())
        }

        path = f'{self.target}/word_list_ru_050922.json'

        with open(path, 'w', encoding='utf-8') as output_file:
            json.dump(word_list, output_file)

        return path


# TODO: –æ–ø–∏—Å–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–π –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
class PickledCorpusReader(HTMLCorpusReader):
    def __init__(self, root, fileids=PKL_PATTERN, **kwargs):
        if not any(key.startswith('cat_') for key in kwargs.keys()):
            kwargs['cat_pattern'] = CAT_PATTERN
        CategorizedCorpusReader.__init__(self, kwargs)
        CorpusReader.__init__(self, root, fileids)

    def docs(self, fileids=None, categories=None):
        fileids = self.resolve(fileids, categories)

        for path in self.abspaths(fileids):
            with open(path, 'rb') as file:
                yield pickle.load(file)

    def paras(self, fileids=None, categories=None):
        for doc in self.docs(fileids, categories):
            for para in doc:
                yield para

    def sents(self, fileids=None, categories=None):
        for para in self.paras(fileids, categories):
            for sent in para:
                yield sent

    def tagged(self, fileids=None, categories=None):
        for sent in self.sents(fileids, categories):
            for tagged_token in sent:
                yield tagged_token

    def words(self, fileids=None, categories=None):
        for tagged in self.tagged(fileids, categories):
            yield tagged[0]


def create_docx(path_to_docx, text_structure, folder_file_name):
    """
    –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ .pickle –∏ —Å –ø–æ—Å–ª–µ–¥—É—é—â–∏–º —Å–æ–∑–¥–∞–Ω–∏–µ–º —Ñ–∞–π–ª–æ–≤ .docx —Å —Ç–µ–∫—Å—Ç–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
    :param path_to_docx:
    :param text_structure:
    :param folder_file_name:
    :return:
    """
    name_list = folder_file_name.split('/')
    folder = name_list[0]
    file = name_list[1].replace('.pickle', '')
    document = Document()
    for ts in text_structure:
        if ts is None:
            document.add_paragraph()
        else:
            for para in ts:
                if para:
                    line = ''
                    for word in para:
                        line += str(f' {word[0]}')
                    document.add_paragraph(line)
    if not os.path.exists(f'{path_to_docx}/{folder}'):
        os.makedirs(f'{path_to_docx}/{folder}')

    if not os.path.isdir(f'{path_to_docx}/{folder}'):
        raise ValueError('–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ —É–∫–∞–∑–∞–Ω –∫–∞—Ç–∞–ª–æ–≥, –∞ –Ω–µ —Ñ–∞–π–ª')

    document.save(f'{path_to_docx}/{folder}/{file}.docx')
    return 0


def resulted_xlsx(path_to_xlsx, fileids, pickled_data):
    """
    –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ .pickle –∏ —Å –ø–æ—Å–ª–µ–¥—É—é—â–∏–º —Å–æ–∑–¥–∞–Ω–∏–µ–º —Ç–∞–±–ª–∏—Ü—ã —Å –∫–ª–∞—Å—Å–∞–º–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ .xlsx
    :param path_to_xlsx:
    :param fileids:
    :param pickled_data:
    """
    resulted_list = []
    site_dict = {
        '3dnews.ru': '3dnews.ru',
        'kuban.rbc.ru': 'kuban.rbc.ru',
        'ria.ru': 'ria.ru',
        'lenta.rupartsnews': 'lenta.ru/parts/news',
        'sports.ru': 'sports.ru',
    }
    filter_list = ['https', '/', 'html', 'meta', 'head', 'xn', 'ria', 'ru', 'internet']

    df = pd.DataFrame(resulted_list, columns=['–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ', '–ò—Å—Ç–æ—á–Ω–∏–∫', '–î–∞', '?', '–ù–µ—Ç'])
    df.to_excel(f'{path_to_xlsx}/resulted_table.xlsx', index=False)

    # if not os.path.exists(f'{path_to_docx}/{folder}'):
    #     os.makedirs(f'{path_to_docx}/{folder}')
    #
    # if not os.path.isdir(f'{path_to_docx}/{folder}'):
    #     raise ValueError('–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ —É–∫–∞–∑–∞–Ω –∫–∞—Ç–∞–ª–æ–≥, –∞ –Ω–µ —Ñ–∞–π–ª')
    #
    # document.save(f'{path_to_docx}/{folder}/{file}.docx')
    return 0


if __name__ == "__main__":
    # TODO: –†–∞–∑–±–∏—Ç—å –ø–æ —Ñ—É–Ω–∫—Ü–∏—è–º
    html_reader = HTMLCorpusReader('./sites', fileids=HTML_PATTERN, cat_pattern=CAT_PATTERN, tags=TAGS)
    # category_type = 'it'
    # fileid = f'./{category_type}/Alphacool_Eisblock_Acryl_GPX_Zotac_RTX_3070_Ti.html'
    # print(html_reader.describe())

    # preproc = Preprocessor(html_reader)
    # started = time.time()
    # preproc.save_tokens()
    # print(time.time() - started)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    # preproc = Preprocessor(html_reader)
    # fileids = preproc.fileids()
    # print('--------------------- HTML to PICKLE ---------------------')
    # for i, fileid in enumerate(fileids):
    #     print(f'{i:6}: {fileid}')
    #     preproc.process(fileid)
    # pass

    # –ß—Ç–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∞—Ä—Ö–∏–≤–∞
    # fileid = f'./{category_type}/Alphacool_Eisblock_Acryl_GPX_Zotac_RTX_3070_Ti.pickle'
    # fileid = f'./3dnews.ru/3dnews.ru1059046obzor-huawei-p50-propage-1.html_1744_09032022.pickle'
    pick = PickledCorpusReader('./processed_corpus', fileids=PKL_PATTERN, cat_pattern=CAT_PATTERN, tags=TAGS)
    fileids = pick.resolve()
    # path_to_docx = './docx_result'
    # print('--------------------- PICKLE to DOCX ---------------------')
    # for i, fileid in enumerate(fileids):
    #     print(f'{i:6}: {fileid}')
    #     text_structure = next(pick.docs(fileid))
    #     create_docx(path_to_docx=path_to_docx, text_structure=text_structure, folder_file_name=fileid)
    # pass

    print('--------------------- PICKLE to XLSX ---------------------')
    path_to_xlsx = './docx_result'
    resulted_xlsx(path_to_xlsx=path_to_xlsx, fileids=fileids, pickled_data=pick)
    pass

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
    # path = f'./processed_corpus/word_list_ru.json'
    # with open(path) as f:
    #     template = json.load(f)
    # print(template)
    # print(len(template['words']))
    # pass

# ----------------------------------------------------------------------------------------------------------------------
# TXT-Reader test
# ----------------------------------------------------------------------------------------------------------------------
# corpus = CategorizedPlaintextCorpusReader('./corpus', DOC_PATTERN, cat_pattern=CAT_PATTERN)
#
# print(corpus.categories())
# print(corpus.fileids())

# txt_reader = TXTCorpusReader('./corpus', fileids=DOC_PATTERN, cat_pattern=CAT_PATTERN)
# # print(txt_reader.resolve())
# category_type = 'auto'
# # print(next(txt_reader.tokenize(fileids=f'./{category_type}/audi_rs4_5.txt')))
# for trt in txt_reader.tokenize(fileids=f'./{category_type}/audi_rs4_5.txt'):
#     print(trt)
# print(next(txt_reader.tokenize(fileids=f'./{category_type}/audi_rs4_5.txt')))
# for tr in txt_reader.words(categories='auto'):
# for tr in txt_reader.sents(fileids='./it/news_nvidia_amd_1.txt'):
# for tr in txt_reader.paras():
#     print(tr)
# print(next(txt_reader.paras()))
# for t in txt_reader.docs():
#     print(t)

# ----------------------------------------------------------------------------------------------------------------------
# HTML-Reader test
# ----------------------------------------------------------------------------------------------------------------------

# html_reader = HTMLCorpusReader('./corpus', fileids=DOC_PATTERN, cat_pattern=CAT_PATTERN, tags=TAGS)
#
# category_type = 'it'
# print(html_reader.describe(fileids=f'./{category_type}/Alphacool_Eisblock_Acryl_GPX_Zotac_RTX_3070_Ti.html'))
